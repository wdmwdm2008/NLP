**https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650408679&idx=1&sn=1e10bf19e59729bedd7407b7e1aa4db1&chksm=becd80bd89ba09ab748973b10ee30c4b0a87b7649de37394abc02de3b061a5853753ae7374c4&scene=21#wechat_redirect**

## 图像领域的预训练
- 为什么预训练是可行的?  
    在CNN中不同层级的神经元学习到不同类型的图像特征，由底向上特征形成层级结构。最底层学习到的是线段等特征，第二层学习到人脸五官的轮廓，第三层学习到的
    是人脸的轮廓。越往上抽取出特征越与手头上的人物相关。尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层
    预训练好的参数初始化新任务网络参数的原因。
- 怎么进行预训练?  
    我们有两种方法：一是frozen。即冻结底层参数，只用少数数据训练顶层网络的参数。二是fine-tuning: 用少数数据训练所有的参数，预训练的参数作为初始化参数。
- 预训练的好处:  
    - 能使用少量数据来训练model
    - 即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，

## 语言模型和word2vec
$$ \begin{equation} \begin{split} P(w_1,...,w_t) = \prod_{i=1}^{t}P(w_i|w_1,...,w_{i-1}) = \prod_{i=1}^{t}P(w_i|Context) \ P(w_1, w_2, …, w_t) 
= P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w_{t-1}) \end{split} \end{equation} $$$

- NNLM(Neural Network Language Model)和word2vec的区别 (Glove也被用于word embedding训练)：
    - NNLM训练方法是输入一个单词的上文来预测这个单词。
    - word2vec训练方法是CBOW和SKIP-GRAM
- 为什么他们训练方法不同？  
    因为NNLM主要任务是学习一个解决语言模型任务网络结构，而word embedding是其的一个副产品。而word2vec主要任务就是word embedding。所以可以随意训练。

- wordEmbedding为什么效果不是特别好？  
    - wordEmbedding能找出语义相近的其他词汇。（相似词）  但是不能解决多义词问题。比如Bank(有河岸，银行两种意思), word2vec对bank进行编码时是不能区
    分这个含义的。因为尽管他们的上下文环境中的单词不同，但是上下文句子经过word2vec编码，都是预测相同的单词bank，而同一个单词占用同一行的参数空间，所
    以其不能区分多义词。
    
## From Word2vec to ELMO(Embedding From Language Model) and Cove
- 提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓。   
- word2vec训练好的word embedding是静态方式。  
- ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备   了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含   义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。  
- ELMO主要进行两个步骤：
    - 一是预训练双层双向的语言模型，这时不仅仅学会单词的word embedding,而且学会了一个双层双向的网络结构。句子中每个单词都对应3个embedding，最低层       是单词的word embedding， 往上走是第一层双向lstm对应单词位置的embedding，这层编码单词的句法信息更多一些，再往上走是第二层lstm对应单词位置的         embedding，这层编码单词的语义信息更多一些。
    - 第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。
- EMLO相比gpr和bert的缺点：
    - LSTM抽取特征能力远弱于transformer
    - 拼接方式双向融合特征融合能力偏弱
    
## 从Word Embedding到GPT
GPT也进行两个阶段：
    - 第一个阶段是利用语言模型进行预训练
    - 第二阶段通过Fine-tuning的模式解决下游任务。
GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：
    - 首先，特征抽取器不是用的RNN，而是用的Transformer
    - 其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型。(在做阅读理解时，其丢失了很多信息)
    
    
## BERT
BERT和GPT的不同是从单向模型变为双向模型。
- NLP的几类问题：
通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。

- 如何改造下游任务？
对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。

- BERT主要创新是masked语言模型(本质是CBOW)和next sentence prediction
    - Masked双向语言模型：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：       训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，       这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被       狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。
    - 至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第       二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子       是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有       助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。
